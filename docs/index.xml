<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome to My Website on Muzhe Wu</title>
    <link>https://wumuzhe.com/</link>
    <description>Recent content in Welcome to My Website on Muzhe Wu</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://wumuzhe.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UIST/ISMAR</title>
      <link>https://wumuzhe.com/news/2024-10-14/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/news/2024-10-14/</guid>
      <description>Had a fruitful two weeks attending UIST and ISMAR and presenting our work New Ears.</description>
    </item>
    <item>
      <title>Performance as Agency? Investigating the Trade-off between Sense of Agency and Performance in Target Selection with Preemptive Assistance in VR</title>
      <link>https://wumuzhe.com/research/agencyperformance/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/research/agencyperformance/</guid>
      <description>&lt;p&gt;Virtual Reality (VR) allows for natural, embodied interactions with virtual elements, fostering immersive experiences. However, these interactions can feel taxing and become more challenging as task demands increase (e.g., selecting small, densely clustered targets). To help users perform such complex tasks, there exist a number of assistance techniques such as providing users with suggestions or automatically performing selections for them. While those techniques increase users’ task performance, their impact on user experience, specifically their Sense of Agency (SoA)—the subjective feeling of controlling one’s actions and their outcomes, is unknown. To investigate the trade-off between task assistance and SoA, we contribute results from a within-subject user study (N=12). We quantitatively examine SoA in VR target selection in relation to task difficulty (2 levels) and assistance (3 levels) which implicitly preempted participants’ actions. We measured SoA with both implicit measures of the intentional binding paradigm (predictive) and participants’ self-reports (postdictive), and compared those to participants’ task performance and behavioral metrics. Our results reveal that while a higher level of assistance improved performance, postdictive SoA remained stable independent of task settings and predictive SoA increased in easy task settings with more assistance. This highlights the feasibility to provide assistance to users in complex tasks, specifically when the assistance technique is designed unobtrusively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>PhD Program Application</title>
      <link>https://wumuzhe.com/news/phd-application/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/news/phd-application/</guid>
      <description>I&amp;rsquo;m applying to PhD programs this cycle (Fall 2025) and would love to chat if there are any opportunities!</description>
    </item>
    <item>
      <title>&#34;Collaborative Early-stage Risk Identification in AI Product Design&#34;</title>
      <link>https://wumuzhe.com/research/ailego/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/research/ailego/</guid>
      <description>&lt;p&gt;Existing Responsible AI (RAI) tools often depend on technical experts to identify harmful problems and are used after the system is built. However, there is growing recognition of the importance of cross-functional collaboration in RAI work during the early design stage. We introduce AI LEGO, a novel interactive tool designed to enhance RAI cross-functional collaboration by helping practitioners communicate and identify harmful design choices early, when problems are easier to address and less likely to cause harm. Through a co-design study with 8 cross-functional AI practitioners and a user study with 18 practitioners, we found that AI LEGO improves practitioners’ ability to identify potential harms in AI designs.  Participants’ feedback also highlights AI LEGO’s effectiveness in facilitating early-stage harm identification across different roles. Finally, we discuss implications for supporting cross-functional collaboration in conducting RAI work in early-stage AI design.&lt;/p&gt;</description>
    </item>
    <item>
      <title>UIST/ISMAR</title>
      <link>https://wumuzhe.com/news/2024-07-30/</link>
      <pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/news/2024-07-30/</guid>
      <description>Finished my master&amp;rsquo;s program at CMU HCII.</description>
    </item>
    <item>
      <title>&#34;AR-enabled Intelligent Tutoring for Rubik’s Cube Learning&#34;</title>
      <link>https://wumuzhe.com/research/rubikon/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/research/rubikon/</guid>
      <description>&lt;p&gt;Learning to solve a Rubik&amp;rsquo;s Cube requires the learners to repeatedly practice a&#xA;skill component, e.g., identifying a misplaced square&#xA;and putting it back. However, for 3D physical tasks such as the Rubik&amp;rsquo;s Cube,&#xA;generating sufficient repeated practice opportunities&#xA;for learners can be challenging, in part because repeated configuration of&#xA;physical objects is strenuous. We propose Rubikon, an&#xA;intelligent tutoring system for learning to solve the Rubik&amp;rsquo;s Cube. Rubikon&#xA;reduces the necessity for repeated manual configurations&#xA;of the Rubik&amp;rsquo;s Cube without compromising the tactile experience of handling a&#xA;physical cube. The foundational design of Rubikon is&#xA;an AR setup, where learners manipulate a physical cube while seeing an&#xA;AR-rendered cube on the screen. Rubikon automatically&#xA;generates configurations of the Rubik&amp;rsquo;s Cube to target learners&amp;rsquo; weaknesses and&#xA;help them exercise diverse knowledge components. A&#xA;between-subjects experiment showed that Rubikon learners scored 25% higher on a&#xA;post-test compared to baselines.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Ears: An Exploratory Study of Audio Interaction Techniques for Performing Search in a Virtual Reality Environment</title>
      <link>https://wumuzhe.com/research/newears/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/research/newears/</guid>
      <description>&lt;p&gt;Efficiently searching and navigating virtual scenes is essential for performing various downstream tasks and ensuring a positive user experience in VR. Prior VR interaction techniques for such scenarios predominantly rely on users’ visual perception, which contrasts with physical reality, where people typically rely on multimodal information, especially auditory cues, to guide their spatial awareness. In this work, we explore the potential of leveraging auditory interaction techniques to enhance spatial navigation in virtualenvironments. We drew inspiration from prior distant interaction techniques and developed four approaches to augmenting how users hear in the virtual environment: Audio Teleportation, Audio Cone, Ninja Ears, and Boom Mic. In a comparative user study (N = 25), we evaluated these approaches against a baseline teleportation technique in a search task, where participants traversed a virtual environment to locate target items. Our results suggest that several ofour audio interaction techniques may enable more efficient search behaviors while enhancing overall user experience. However, not all techniques were appreciated equally, suggesting that careful attention to their design is critical for ensuring their effectiveness. We conclude by discussing the potential implications of our results for future audio interaction technique designs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mini-World: Ambient Co-presence in Web Browsing</title>
      <link>https://wumuzhe.com/projects/miniworld/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/miniworld/</guid>
      <description>&lt;div style=&#34;display: block; text-align: center;&#34; class=&#34;pt-2&#34;&gt;&#xA;    &lt;video muted autoplay loop playsinline style=&#34;width: 80%; height: auto; object-fit: cover;&#34;&gt;&#xA;        &lt;source src=&#34;https://wumuzhe.com/images/projects/MiniWorld/teaser.mp4&#34; type=&#34;video/mp4&#34;&gt;&#xA;        Your browser does not support the video tag.&#xA;    &lt;/video&gt;&#xA;&lt;/div&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Ambient co-presence&lt;/strong&gt; is the feeling of sharing a space or context with others without directly interacting or constantly communicating. In physical settings, peripheral vision, ambient noise, and embodied sensations help us remain aware of one another—cues that are largely missing in digital environments. This project explores how to recreate ambient co-presence during everyday web browsing. I focused specifically on &lt;strong&gt;avatars&lt;/strong&gt;—digital representations of ourselves—and their potential to heighten the sense of shared presence. Existing tools (e.g., Google Docs) already show simple “head icons,” but these are often limited to small interface elements. In this project, I delved into the advantages and challenges of potentially trancending this boundary, anchoring avatars on the users&amp;rsquo; cursor in the browsing experience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM-enhanced Casual Learning Experience on Wikipedia</title>
      <link>https://wumuzhe.com/projects/wikilearn/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/wikilearn/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;Wikipedia is a vast, crowdsourced knowledge base that many people turn to for &lt;strong&gt;casual learning&lt;/strong&gt;—quick, spontaneous exploration of new information. However, it can be overwhelming and sometimes questioned for its reliability. Meanwhile, Large Language Models (LLMs) provide instant, personalized text generation but face criticism for fabricating or misrepresenting facts. This project explores how LLMs and Wikipedia can work together to create a more seamless, trustworthy, and engaging experience for casual learners. By integrating LLM-based features directly into Wikipedia articles, the goal is to support readers in grasping complex information more easily, while maintaining the spontaneity and depth that makes Wikipedia so appealing for informal discovery.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Personified Goal Tracking Experience with Decreasing Obtrusiveness</title>
      <link>https://wumuzhe.com/projects/xrgoaltracker/</link>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/xrgoaltracker/</guid>
      <description>&lt;figure&gt;&#xA;    &lt;img &#xA;        src=&#34;https://wumuzhe.com/images/projects/GoalTracking/teaser_large.png&#34; &#xA;        alt=&#34;XR Goal Tracker teaser&#34; &#xA;        style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;        class=&#34;pt-2&#34;&gt;&#xA;    &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;Many people struggle to achieve personal goals, whether it’s eating healthier, exercising consistently, or staying organized with daily tasks. This project introduces a prototype for an active and immersive goal-tracking experience, featuring a goal tracker personified as celebrity avatars the user may feel connected to. Inspired by the concept of &lt;strong&gt;fading&lt;/strong&gt;, the tracker gradually becomes less obstrusive as the user establishes consistent habits. By implementing this in VR, I explored whether extended, immersive interactions with celebrity avatars could enhance user motivation and engagement.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ActiveAI: The Effectiveness of an Interactive Tutoring System in Developing K-12 AI Literacy</title>
      <link>https://wumuzhe.com/research/activeai/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/research/activeai/</guid>
      <description>&lt;p&gt;As we witness groundbreaking advancements in Artificial Intelligence (AI), it is clear that the next generation must be equipped with AI literacy: the skill to interact, evaluate, and collaborate with AI systems. This study introduces ActiveAI, a scalable web-based tutoring system aligned with AI4K12’s five big ideas in AI, designed to foster AI literacy among K-12 students through active learning and interaction with intelligent agents. A controlled classroom study involving 171 mid- dle school learners was conducted to assess the effectiveness of ActiveAI in fostering AI literacy skills and competency toward AI. Results showed that, compared to students in the tell-and-practice control condition, stu- dents who used ActiveAI exhibited higher post-test performance in the module about how next-word prediction and temperature work in large language models. Students also developed higher self-reported compe- tence toward AI after using ActiveAI than in the control condition. We conclude by suggesting assessment designs that promote deeper engage- ment with AI concepts by addressing students’ common misconceptions, like “AI thinks just like humans”, in K-12 AI literacy education.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How Do You See The World: Visual Impairment Simulations in VR</title>
      <link>https://wumuzhe.com/projects/howtoseetheworld/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/howtoseetheworld/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; margin-bottom: 2em; height: 0; overflow: hidden; max-width: 100%; background: #000;&#34;&gt;&#xA;    &lt;iframe &#xA;        src=&#34;https://www.youtube.com/embed/G-AvFl6b4iE&#34; &#xA;        style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; &#xA;        frameborder=&#34;0&#34; &#xA;        allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; &#xA;        allowfullscreen&gt;&#xA;    &lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;People with normal vision often lack an understanding of the daily challenges faced by individuals with &lt;strong&gt;visual impairments&lt;/strong&gt;. This gap in understanding perpetuates barriers to empathy, inclusivity, and accessible design. To address this, this project immerses participants in a hands-on experience that simulates these challenges through virtual environments, interactive tasks, and reflective debriefings, promoting empathy and driving more inclusive practices in technology and beyond.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Auxiliary Variables Improve Group Accuracy without Group Information</title>
      <link>https://wumuzhe.com/projects/spuriouscorrelation/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/spuriouscorrelation/</guid>
      <description>&lt;figure&gt;&#xA;    &lt;img &#xA;        src=&#34;https://wumuzhe.com/images/projects/SpuriousCorrelation/teaser_large.png&#34; &#xA;        alt=&#34;Spurious Correlation teaser&#34; &#xA;        style=&#34;max-width: 80%; display: block; margin: 0 auto;&#34; &#xA;        class=&#34;pt-2&#34;&gt;&#xA;    &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;&#xA;&lt;p&gt;Neural networks often exhibit poor accuracy on rare subgroups despite achieving high overall accuracy due to &lt;strong&gt;spurious correlations&lt;/strong&gt;. This occurs when the model relies on superficial features for decision-making, leading to unintended behaviors for certain subgroups within the data distribution. This phenomenon is widespread across various fields, including computer vision, natural language processing, and reinforcement learning.&lt;/p&gt;&#xA;&lt;p&gt;Prior research addressing spurious correlations primarily relied on group annotations for either the entire training dataset or only the validation set. These methods, while effective, are often impractical due to the cost and difficulty of obtaining such annotations for real-world datasets. The Bias Amplification (BAM) algorithm, introduced in a &lt;a href=&#34;https://arxiv.org/abs/2309.06717&#34;&gt;prior work&lt;/a&gt;, aims to improve group robustness without relying on group annotations. This algorithm employs a two-stage training process:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Antiwork: Work-Related Stress Identification and Leading Factor Analysis</title>
      <link>https://wumuzhe.com/projects/whyantiwork/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/whyantiwork/</guid>
      <description>&lt;figure&gt;&#xA;    &lt;img &#xA;        src=&#34;https://wumuzhe.com/images/projects/WhyAntiwork/teaser_large.png&#34; &#xA;        alt=&#34;WhyAntiwork teaser&#34; &#xA;        style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;        class=&#34;pt-2&#34;&gt;&#xA;    &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;Toxic workplaces can contribute to mental health problems like anxiety, depression, and even suicidal ideation. Some of the reasons employees may develop negative feelings towards work are unpleasant working environments, unreasonable workloads, and harsh supervisors. These feelings are referred to as &amp;ldquo;antiwork sentiment&amp;rdquo; and can result in harmful negative emotions. This project examines the characteristics and underlying causes of antiwork sentiment and explores methods for predicting such sentiment. The goal is to provide insights into reducing workplace toxicity and enabling timely interventions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Feature Alignment Discriminator for Text Summarization</title>
      <link>https://wumuzhe.com/projects/fad/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/fad/</guid>
      <description>&lt;figure&gt;&#xA;    &lt;img &#xA;        src=&#34;https://wumuzhe.com/images/projects/FAD/teaser_large.png&#34; &#xA;        alt=&#34;FAD teaser&#34; &#xA;        style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;        class=&#34;pt-2&#34;&gt;&#xA;    &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Text summarization&lt;/strong&gt; is a critical task in NLP that involves extracting the most important information from a text to create a concise version tailored to a specific purpose.&#xA;While extractive methods select and combine existing phrases from the text, abstractive summarization generates summaries using rephrased wording based on a high-level understanding of the content. Despite its alignment with human thought processes, abstractive summarization often struggles to produce coherent and precise summaries.&#xA;For example, state-of-the-art (SOTA) models like BART (at the time) may include irrelevant or inaccurate information, resulting in inconsistencies in the generated summaries. This project explores the use of a &lt;strong&gt;feature alignment discriminator (FAD)&lt;/strong&gt; to enhance the quality of abstractive summarization.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mask Redistribution Simulator</title>
      <link>https://wumuzhe.com/projects/maskredistribution/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/maskredistribution/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;I started learning computer programming during the devastating COVID-19 outbreak. My teammates and I were deeply shocked to learn about the numerous deaths in Hubei Province caused by sudden infection spikes and mask shortages. This inspired us to create a project in C++ with OpenGL to simulate how varying production capacities, transportation routes, and population behaviors might affect the spread of the virus. Our goal was to predict a more efficient mask distribution network for Hubei Province, China.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Other Stuff I Made</title>
      <link>https://wumuzhe.com/projects/other/</link>
      <pubDate>Wed, 22 Aug 2001 00:00:00 +0000</pubDate>
      <guid>https://wumuzhe.com/projects/other/</guid>
      <description>&lt;p&gt;Over the years, I’ve been fortunate to build a variety of fun projects. This page highlights a few of them with brief overviews and photos. Hopefully, they can offer a window into how my thinking has evolved around different materials and media.&lt;/p&gt;&#xA;&lt;h3 id=&#34;lego-mindstorms-nxt&#34;&gt;LEGO Mindstorms NXT&lt;/h3&gt;&#xA;&lt;p&gt;I used to be a competitive LEGO Mindstorms NXT player and won a few robotics competitions (e.g., national champion at &lt;a href=&#34;https://www.robocup.org/&#34;&gt;RoboCup&lt;/a&gt;, &lt;a href=&#34;https://apyrc.tumblr.com/&#34;&gt;APRC&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;div class=&#34;slider-container&#34;&gt;&#xA;  &lt;button class=&#34;slider-button left&#34;&gt;&amp;#10094;&lt;/button&gt;&#xA;  &lt;div class=&#34;slider-track&#34;&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/lionDance2.jpg&#34; &#xA;              alt=&#34;lion dance robot&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/lionDance1.jpg&#34; &#xA;              alt=&#34;lion dance robot&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/lionDance3.jpg&#34; &#xA;              alt=&#34;lion dance robot&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/lionDance4.png&#34; &#xA;              alt=&#34;lion dance robot&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/RoboCup.jpg&#34; &#xA;              alt=&#34;RoboCup photo&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/tractor.png&#34; &#xA;              alt=&#34;tractor&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/gridClimbing.jpeg&#34; &#xA;              alt=&#34;grid climbing robot&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;slider-item&#34;&gt;&#xA;      &lt;figure&gt;&#xA;          &lt;img &#xA;              src=&#34;https://wumuzhe.com/images/projects/Other/Robots/pattern.jpg&#34; &#xA;              alt=&#34;pattern&#34; &#xA;              style=&#34;max-width: 100%; display: block; margin: 0 auto;&#34; &#xA;              class=&#34;pt-2&#34;&gt;&#xA;          &lt;figcaption&gt;&lt;/figcaption&gt;&#xA;      &lt;/figure&gt;&#xA;    &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;button class=&#34;slider-button right&#34;&gt;&amp;#10095;&lt;/button&gt;&#xA;&lt;/div&gt;&#xA;&lt;h3 id=&#34;movable-bridge&#34;&gt;Movable Bridge&lt;/h3&gt;&#xA;&lt;p&gt;In SJTU VG100 Intro to Engineering, our team prototyped a wooden drawbridge that could retract in an old-fashioned style.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
